\documentclass[a4paper,10pt]{article}
\pdfoptionpdfminorversion=5
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{multicol}
\usepackage{xcolor,colortbl}
\usepackage{datetime}
\usepackage{xfrac}

\newcommand{\mc}[2]{\multicolumn{#1}{c}{#2}}
\definecolor{Gray}{gray}{0.85}
\newcolumntype{a}{>{\columncolor{Gray}}c}
\newcolumntype{b}{>{\columncolor{white}}c}
\newcolumntype{R}{>{\columncolor{lime}}c}
\newcolumntype{B}{>{\columncolor{pink}}c}
\usepackage{diagbox}

\usepackage[numbers]{natbib}

\usepackage[utf8]{inputenc}
\usepackage{array,multirow,graphicx}
\usepackage{graphics}
\usepackage{color}
\usepackage{ifpdf}
\ifpdf
\DeclareGraphicsRule{*}{mps}{*}{}
\fi
\usepackage[utf8]{inputenc}

\hoffset = 0pt
\voffset = 0pt
\oddsidemargin = 0pt
\headheight = 15pt
\footskip = 30pt
\topmargin = 0pt
\marginparsep = 5pt
\headsep =25pt
\marginparwidth = 54pt
\marginparpush = 7pt
\textheight = 621pt %621/
\textwidth = 500pt

% Default fixed font does not support bold face
\DeclareFixedFont{\ttb}{T1}{txtt}{bx}{n}{12} % for bold
\DeclareFixedFont{\ttm}{T1}{txtt}{m}{n}{12}  % for normal

\newcommand{\I}{\ensuremath{\mathbb{I}}}
\newcommand{\R}{\ensuremath{\mathbb{R}}}

\newcommand{\norm}[1]{\ensuremath{\left\|#1\right\|}}
\newcommand{\seminorm}[1]{\ensuremath{\left|#1\right|}}
\newcommand{\dual}[2]{\ensuremath{\langle#1, #2 \rangle}}

\newcommand{\inner}[2]{\ensuremath{\left(#1, #2\right)}}
\newcommand{\Inner}[2]{\ensuremath{\left(\left(#1, #2\right)\right)}}

\newcommand{\deriv}[2]{\ensuremath{\frac{\mathrm{d}#1}{\mathrm{d}#2}}}
\newcommand{\meas}[1]{\ensuremath{\,\mathrm{d}#1}}
\newcommand{\Div}[1] {\ensuremath{\text{div}#1}}
\newcommand{\Grad}[1]{\ensuremath{\text{grad}#1}}
\newcommand{\Curl}[1]{\ensuremath{\text{curl}#1}}
\newcommand{\jump}[1]{\ensuremath{[\![#1]\!]} }

\newcommand{\citeneed}{\ensuremath{^\text{\textcolor{blue}{cite?}}}}

\usepackage{mathtools}
\newcommand{\eqdef}{\mathrel{\mathop=}:} 

\newcommand{\mm}{\ensuremath{\mathbf{m}}}
\newcommand{\Vp}{\ensuremath{V_{\mathcal{P}}}}
\newcommand{\Vb}{\ensuremath{V_{\mathcal{B}}}}
\newcommand{\Ep}{\ensuremath{E_{\mathcal{P}}}}
\newcommand{\Eb}{\ensuremath{E_{\mathcal{B}}}}

%matrices
\newcommand{\Ap}{\ensuremath{\mathbb{A}_{\mathcal{P}}}}
\newcommand{\Ab}{\ensuremath{\mathbb{A}_{\mathcal{B}}}}
\newcommand{\Bp}{\ensuremath{\mathbb{B}_{\mathcal{P}}}}
\newcommand{\Bb}{\ensuremath{\mathbb{B}_{\mathcal{B}}}}
\newcommand{\Amat}{\ensuremath{\mathbb{A}}}
\newcommand{\Bmat}{\ensuremath{\mathbb{B}}}
\newcommand{\Bmatt}{\ensuremath{\mathbb{B}^{\text{T}}}}
\newcommand{\Mmat}{\ensuremath{\mathbb{M}}}
\newcommand{\Nmat}{\ensuremath{\mathbb{N}}}

\newcommand{\Ammatt}{\ensuremath{\tilde{\mathbb{A}}_m}}              %
\newcommand{\Mmmatt}{\ensuremath{\tilde{\mathbb{M}}_m}}              %
\newcommand{\Ammat}{\ensuremath{\mathbb{A}_m}}               %
\newcommand{\Mmmat}{\ensuremath{\mathbb{M}_m}}               %
\newcommand{\Pmat}{\ensuremath{\mathbb{P}_m}}                  %
\newcommand{\Pmatt}{\ensuremath{\mathbb{P}_m^{\text{T}}}}       %

%components
\newcommand{\Apij}[2]{\ensuremath{\left(\mathbb{A}_{\mathcal{P}}\right)_{#1, #2}}}
\newcommand{\Abij}[2]{\ensuremath{\left(\mathbb{A}_{\mathcal{B}}\right)_{#1, #2}}}
\newcommand{\Bpij}[2]{\ensuremath{\left(\mathbb{B}_{\mathcal{P}}\right)_{#1, #2}}}
\newcommand{\Bbij}[2]{\ensuremath{\left(\mathbb{B}_{\mathcal{B}}\right)_{#1, #2}}}
%vectors
\newcommand{\Uvec}{\ensuremath{\mathbf{U}}}
\newcommand{\bvec}{\ensuremath{\mathbf{b}}}
\newcommand{\Pvec}{\ensuremath{\mathbf{P}}}

\newcommand{\up}{\ensuremath{u_{\mathcal{P}}}}
\newcommand{\ub}{\ensuremath{u_{\mathcal{B}}}}
\newcommand{\vp}{\ensuremath{v_{\mathcal{P}}}}
\newcommand{\vb}{\ensuremath{v_{\mathcal{B}}}}
\newcommand{\TODO}[1]{\textcolor{red}{#1}}
\newcommand{\ASK}[1]{\textcolor{blue}{#1}}

\newcommand{\W}[1]{\ensuremath{w\!\left[#1\right]\!}}
\newcommand{\E}[1]{\ensuremath{\epsilon \!\left[#1\right]\!}}
\newcommand{\T}[1]{\ensuremath{\sigma \! \left[#1\right]\!}}

\newcommand{\Tr}[1]{\ensuremath{\text{tr}#1}}
\newcommand*{\defeq}{\mathrel{\vcenter{\baselineskip0.5ex \lineskiplimit0pt
                     \hbox{\scriptsize.}\hbox{\scriptsize.}}}%
                     =}
\newcommand\mapsfrom{\mathrel{\reflectbox{\ensuremath{\mapsto}}}}
\usepackage{lipsum}
\usepackage{authblk}
\usepackage{fancyhdr}
\usepackage[colorlinks=true,
            linkcolor=cyan,
            citecolor=gray]{hyperref}

\usepackage{chngcntr}
%\counterwithin{table}{subsection}
%

\DeclareMathOperator{\spn}{span}

% DEBUGGING
\usepackage{lineno}
\linenumbers
%\usepackage{setspace}
%\doublespacing
%
%\pagestyle{fancy}
%
\renewenvironment{abstract}{%
\hfill\begin{minipage}{0.95\textwidth}
\rule{\textwidth}{1pt}}
{\par\noindent\rule{\textwidth}{1pt}\end{minipage}}
%
\makeatletter
\renewcommand\@maketitle{%
\hfill
\begin{minipage}{0.95\textwidth}
\vskip 2em
\let\footnote\thanks 
{\LARGE \@title \par }
\vskip 1.5em
{\large \@author \par}
\end{minipage}
\vskip 1em \par
}
\makeatother
%
\begin{document}
%
%title and author details
\title{\begin{center}
        Connecting FEM and spectral methods
       \end{center}}
\author[1]{MK}
%\author[2]{Name2}
%\affil[1]{Address of author} 
%\affil[2]{Address of second author}
%
\maketitle
%
\begin{abstract}
  For one-dimensional Poisson problem we study a connection between the
  stiffness and mass matrices obtained from a discretization with the finite
  element method using continuous linear Lagrange elements and two discretizations
  with the Galerkin method where the eigenfunctions of the Laplacian and
  combinations of Legendre polynomials are used respectively as the test
  functions. We show that the matrices due to finite element method can be
  viewed as the limit of transformations describing the change of basis. Further,
  we study the limiting behaviour and show that the two basis lead to same rate
  of convergence despite having different approximation properties.
  Finally properties of the transformation between spaces spanned by the
  global functions (eigenfunctions, polynomials) and the space spanned by 
  local (finite element) functions are studied.
\end{abstract}

\section{Introduction}
  % explaing that the methods start from variational formulation, solve that
  % problem by Galerkin method, subspace and lead to matrices. The differences
  % are in how the subspace is constraucted naturally leads to different
  % matrices
  For given data $f$ we consider a problem of finding sufficiently regular
  function $u$ that has zero boundary values and satisfies
  \begin{equation}
    \label{eq:strong}
    u^{\prime\prime} = f
  \end{equation}
  inside the domain $\Omega=\left[-1, 1\right]$. Numerical methods studied here
  are based on a variational formulation of (\ref{eq:strong}) which reads: Find
  $u\in V=H^1_0(\Omega)$ such that
  \begin{equation}
    \label{eq:weak}
    \inner{u^{\prime}}{v^{\prime}} = \inner{f}{v}\quad\forall v\in V.
  \end{equation}
  The methods then solve (\ref{eq:weak}) approximately by the Galerkin method,
  that is, by considering the problem on a finite dimensional subspace
  $V_h\subset V$. If $V_h=\spn\left\{\phi_i\right\}_{i=0}^{n-1}$ the Galerkin
  method leads to a linear system
  \[
    \Amat\Uvec = \bvec
  \]
  for the unknown expansion coefficients of the solution
  $u_h=\sum_{i=0}^{n-1}\Uvec_i \phi_i$. The matrix $\Amat\in\R^{n\times n}$ is
  called the stiffness matrix and has the entries given by
  \[
    \Amat_{i, j} = \inner{\phi_j^{\prime}}{\phi_i^{\prime}}.
  \]
  Components of the vector $\bvec\in\R^n$ are $\bvec_i=\inner{f}{\phi_i}$. We
  note that if $V_h\ni f=\sum_{i=0}^{n-1}\mathbf{F}_i \phi_i$, then
  $\bvec=\Mmat\mathbf{F}$. Here we have introduced the mass matrix $\Mmat$ whose
  components are given by
  \[
    \Mmat_{i, j} = \inner{\phi_j}{\phi_i}.
  \]
  It is clear that properties of mass and stiffness matrices depend on the
  choice of basis functions of $V_h$. We shall briefly summarize how the finite
  dimensional subspace is constructed in each of the methods.
  % For each.  V_h, A, M, convergence properties of the solution! 
  \subsection{Finite element method}
  For given $n\in\mathbb{N}, n>1$, we shall partition a bi-unit interval into
  $n+1$ elements $e_i=\left[x_i, x_{i+1}\right], i=0, 1, \dots, n$ where the
  vertices $x_i$ are such that $-1\leq x_i\leq-1$ for $i=0, 1, \dots, n+1$. Using
  vertices $x_i$ we define for $i=1,2\dots,n$ the hat-functions $\phi_i$,
  \begin{equation}
    \label{eq:hat}
    \phi_i(x)=\begin{cases}
      \frac{x-x_{i-1}}{x_i-x_{i-1}}&\quad x\in e_{i-1},\\
      \frac{x-x_{i+1}}{x_i-x_{i+1}}&\quad x\in e_{i},\\
      0&\quad\text{otherwise}.
               \end{cases}
  \end{equation}
  For notational simplicity we shift the counting index $i\leftarrow i-1$ and
  define $V_n=\spn\left\{\phi_i\right\}_{i=0}^{n-1}$. Clearly we have
  $\text{dim}V_n=n$ and $V_n\subset V$. If the vertices $x_i$ are equidistant
  with spacing $h=\tfrac{2}{n+1}$ the stiffness matrices can be immediately
  written down. Indeed we have
  \[
    \Amat = \frac{1}{h}\begin{bmatrix}
      2 & -1 & 0 & 0 & 0 &\dots & 0 \\
      -1 & 2 & -1 & 0 & 0 &\dots & 0 \\
      0  & -1 & 2 & -1 & 0&\dots & 0 \\
      0  & 0 & \ddots & \ddots & \ddots &\dots & 0\\
      0 & \dots & 0 & -1 & 2 & -1 & 0 \\
      0 & \dots & 0 & 0 & -1 & 2 & -1 \\
      0 & \dots & 0 & 0 & 0 & -1 & 2 \\
          \end{bmatrix}
  \]
  and
  \[
    \Mmat = \frac{h}{6}\begin{bmatrix}
      4 & 1 & 0 & 0 & 0 &\dots & 0 \\
      1 & 4 & 1 & 0 & 0 &\dots & 0 \\
      0  & 1 & 4 & 1 & 0&\dots & 0 \\
      0  & 0 & \ddots & \ddots & \ddots &\dots & 0\\
      0 & \dots & 0 & 1 & 4 & 1 & 0 \\
      0 & \dots & 0 & 0 & 1 & 4 & 1 \\
      0 & \dots & 0 & 0 & 0 & 1 & 4 \\
          \end{bmatrix}
  \]
  Finally we note that the following error estimate holds for the method 
  $\norm{u^{\prime}-u_h^{\prime}}\leq h\norm{u^{\prime}}_2$. The error therefore
  decreases linearly if the mesh size is decreased (or equivalently $n$ is
  increased).

  \subsection{Galerkin method with eigenfunctions}
  We let $\alpha_k=\frac{\pi}{2}+k\frac{\pi}{2}$ for $k=0, 1, \dots$ and
  consider functions $\varphi_k$,
  \[
    \varphi_k(x) = \begin{cases}
                \cos{\alpha_k x} &\quad k \text{ is even },\\
                \sin{\alpha_k x} &\quad k \text{ is odd }.\\
              \end{cases}
  \]
  Clearly $\varphi_k\in V$. Moreover with $\lambda_j=\alpha_j^2$, the functions
  $\varphi_j$ are solutions to the eigenvalue problem: Find $\lambda\in\R, u\in V$
  such that
  \[
    \inner{u^{\prime}}{v^{\prime}} = \lambda \inner{u}{v}\quad\forall v \in V.
  \]
  Finally we note that $\inner{\varphi_i}{\varphi_j}=\delta_{ij}$ and 
  $\inner{\varphi_i^{\prime}}{\varphi_j^{\prime}}=\lambda_j\delta_{ij}$. The
  functions are thus orthonormal in the $L^2$-inner product and orthogonal in
  the $H^1$ inner product. The set $\left\{\varphi_k\right\}_{k=0}^{\infty}$
  forms an orthogonal basis of $V$. In the Galerkin method we shall define the
  finite dimensional space as $V_m=\spn\left\{\varphi_i\right\}_{i=0}^{m-1}$.
  Owing to the orthogonality properties the stiffness and mass matrices
  assembled on $V_m$ take a particularly simple form. We have
  \begin{equation}
    \label{eq:shen_mat}
    \Amat_{ij} = \lambda_j\delta_{ij}
    \quad\text{ and }\quad
    \Mmat_{ij} = \delta_{ij}.
  \end{equation}
  Finally we note that the following error estimate holds for the method 
  $\norm{u^{\prime}-u_m^{\prime}}\leq \frac{1}{\alpha_m}\norm{f}$ and thus
  we expect the error in the energy norm to decrease linearly with increasing
  $m$.

  \subsection{Galerkin method with Legendre polynomials}
  Let us denote $L_k$ the $k$-th Legendre polynomial. It is known
  (e.g. \cite{shen_book}) that the boundary values of Legendre polynomials are
  \[
    L_k(1) = 1\quad\text{ and }\quad L_k(-1) = (-1)^k
  \]
  and consequently $l_k\notin V$. However special combinations of Legendre
  polynomials can be taken to yield polynomials that have zero boundary values.
  If $\varphi_k=\beta_k\left(L_{k+2} - L_k\right), k=0, 1,\dots$ with constants
  $\beta_k$ given as $\beta_k = \frac{1}{\sqrt{4k + 6}}$ then
  \cite{shen_leg} showed that the functions are zero on the boundary and
  consequently $\varphi_k\in V$. The functions $\varphi_j$ have desirable
  orthogonality properties, namely
  $\inner{\varphi_i^{\prime}}{\varphi_j^{\prime}}=\delta_{ij}$ and
  $\inner{\varphi_i}{\varphi_j}=0$ for $j\notin\left\{i-2, i, i+2\right\}$.
  For the Galerkin method we set $V_m=\spn\left\{\varphi_i\right\}_{i=0}^{m-1}$
  so that $\text{dim}V_m=m$. As with the eigenfunctions, the matrices assembled
  on $V_m$ have a simple form. The stiffness matrix is in fact an identity
  while the mass matrix is sparse tridiagonal matrix with nonzero entries
  \[
    \Mmat_{ii} = \beta_i^2\left(\frac{2}{2i+1}+\frac{2}{2i+5}\right)
    \quad{\text{ and }}\quad
    \Mmat_{i(i+2)} = -\beta_i\beta_{i+2}\frac{2}{2(i+2)+1}.
  \]
  Finally, owing to the error estimate (\cite{shen_leg})
  \[
    \norm{u-u_m} + (m+1)\norm{u-u_m}_1 \leq C(s)(m+1)^{-s}\norm{u}_s
  \]
  the method converges exponentially for $u\in H^s\left({\Omega}\right)$.

  With the stiffness and mass matrices of all methods in place we shall now
  try to answer a question whether there exists a connections between the
  matrices of the finite element method and the matrices of the other two
  methods. Broadly speaking we are asking for existence of the transformation
  (and possibly its properties) of the global method (in a sense that the
  support of neither eigenfunctions nor the polynomials due to Shen is localized)
  and the local (finite element) method.

  % Now we are interested in connection between FEM(local) and other global
  % matrices/methods
  \section{Eigen $\rightarrow$ FEM}
  Consider the finite dimensional space of eigenfunctions $V_m$ and function $f$
  which has zeros on the boundary and is at least $C^0$. We then define $\pi_m f$
  as
  \[
    \pi_m f = \displaystyle\sum\limits_{k=0}^{m-1}c_k \varphi_k,
  \]
  where the coefficients $c_k$ are $c_k=\inner{f}{\phi_k}$. Function $\pi_m$ is
  the usual $L^2$ projection of $f$ onto $V_m$. To test the approximation
  properties of the projection, we have considered functions
  $f_0=e^{2x}(x^4-1)$ and $f_1=e^{2x}(x^4-1)\sin{4\pi x}$ and measured
  convergence rate of $f_i - \pi_{f_i}$ in the $L^2, H^1_0$ norms. The results
  are shown in Figure \ref{fig:eig_smooth_projection}. We see that for the
  function $f_0$ which does not oscillate inside the interval, the rate is constant
  (by the least square fit we obtained $m^{-2.15}$ in the $L^2$ nom and $m^{-1.25}$
  in the energy norm by the least squares fit). On the other hand the rates
  for function $f_1$ behave as $m^{-2}$ and $m^{-1}$ only for $m>10$. The key
  to this behavior is in the right pane of the figure which shows the spectrum
  of function $f_0, f_1$. The first function has a smooth spectrum where the the
  megnitude of coefficinets decays for all $k$ ($k^{-2.78}$ by the least squares
  fit) while the second spectrum decays monotonically only for $k>10$.
  \begin{figure}
  \begin{center}
    \includegraphics[width=0.45\textwidth]{img/eigen_smooth_rate.pdf}
    \includegraphics[width=0.45\textwidth]{img/eigen_smooth_power.pdf}
  \end{center}
  \label{fig:eig_smooth_projection}
  \caption{Convergence rate of $\pi f$ for smooth functions. (left) The
  convergence rate in the $L^2$ norm(squares) and $H^1_0$ norm(circles). 
  (right) Magnitude of coefficients $c_k$.}
  \end{figure}

  Since our main interest lies in transformation to finite element method we
  now consider properties of the projection $\pi_m$ for the hat functions
  $\phi_i$( cf. eq (\ref{eq:hat})). In general, for hat function $\phi_i$ we
  have the expansion coefficients of projection $c_k$ given by
  \[
    c_k = \inner{\phi_i}{\varphi_k} =
    \displaystyle\int_{x_{i-1}}^{x_{i}}\frac{x-x_{i-1}}{x_i-x_{i-1}}\varphi_k(x)\meas{x}+
    \displaystyle\int_{x_{i}}^{x_{i+1}}\frac{x-x_{i+1}}{x_i-x_{i+1}}\varphi_k(x)\meas{x}.
  \]
  The above integral can be carried by the integration by parts with the result
  \begin{equation}
    \label{eq:eigen_hat_ck}
    c_k =
    \frac{1}{\alpha_k^2}
    \left(
    \frac{\varphi_k(x_i) - \varphi_k(x_{i-1})}{x_i - x_{i-1}} +
    \frac{\varphi_k(x_{i+1}) - \varphi_k(x_{i})}{x_i - x_{i+1}}
    \right)
  \end{equation}
  To analyze the asymptotic behaviour of $c_k$ we shall consider homogeneous
  mesh with elements of size $h$ and consider special hat functions centered
  at the origin. We shall refer to these as $(-h, 0, h)$. This choice greatly
  simplifies the calculations but it is by no means unreasonable as for the mesh
  with even number of elements such function is always present in $V_n$.

  Since functions $(-h, 0, h)$ are even we have $c_k=0$ for $k$ odd (recall that
  odd wavenumbers belong to sines). For $k$ even expression
  (\ref{eq:eigen_hat_ck}) simplifies to
  \[
    c_k = \frac{4}{\alpha_k^2 h}\sin^2\left(\frac{h}{2}\alpha_k\right).
  \]
  Finally by considering the term
  $\frac{\sin^2\left(\frac{h}{2}\alpha_k\right)}{h}$
  we obtain for $\frac{h}{2}\alpha_k > \frac{\pi}{2}$ or equivalently 
  $k+1>\tfrac{2}{h}$ the bound on the coefficients
  \[
    c_k \leq \frac{1}{\alpha_k^2}.
  \]
  This implies that for each $h$ there exists a critical wavenumber $k_0=2/h$
  such that for all wavenumbers $k+1\geq k_0$ the coefficients $c_k$ will always
  be  smaller than $k^{-2}$. For the future reference we note that $k_0=n+1$.

  The estimate of the critical wavenumber as well as the asymptotic behaviour
  agree well with the numerical experiments whose results are shown in Figure
  \ref{fig:eig_hat_spectrum}. Note that the right pane of the figure showing
  the spectrum of shifted hat functions $(-h+0.1, 0, h)$ also shows the
  asymptotic decay $k^{-2}$.
  \TODO{Maybe add the error plots?}
  \TODO{General estimates for homogeneous mesh.}
  \TODO{There must be a convergence theory for this series.}

  \begin{figure}
  \begin{center}
    \includegraphics[width=0.45\textwidth]{img/eigen_hat_spectrum_centered.pdf}
    \includegraphics[width=0.45\textwidth]{img/eigen_hat_spectrum_off.pdf}
  \end{center}
  \label{fig:eig_hat_spectrum}
  \caption{Spectrum of has function. (left) Dependence of $\inner{f}{\varphi_k}$
    on the wave number for the hat function $(-h, 0, h)$. Vertical lines denote
    the computed critical wave number $k_0$. (right) Dependence of coefficients
    $c_k$ on the wave number for hat function $(-h+0.1, 0.1, h+0.1)$. Note that
    $c_k=0$ were excluded from the plot. Both plots confirm the $k^{-2}$ decay.}
  \end{figure}

  Let us denote $V_n, V_m\subset V$ the finite dimensional subspaces of $V$
  constructed respectively by $n$ hat functions $\phi_i$ and by first $m$
  eigenfunctions $\varphi_j$. Further let $\Amat, \Mmat \in \R^{n \times n}$ denote
  the stiffness and mass matrices assembled with on $V_n$. Finally we denote
  by $\Ammatt, \Mmmatt \in \R^{m\times m}$ the stiffness and mass matrices
  assembled on $V_m$. In the previous section we showed boundedness of the
  mapping $\phi_i\ni V_n \mapsto \pi_m\phi_i \in V_m$.
  We shall represent this mapping by a matrix $\Pmatt\in\R^{m x\times n}$ whose
  colums have expansions coefficients $c_k, k=0,1, \dots m-1$ for all $n$
  functions in $V_n$ and define two $n$ by $n$ matrices
  \[
    \Mmmat = \Pmat\Mmmatt\Pmatt
    \quad{\text{ and }}\quad
    \Ammat = \Pmat\Ammatt\Pmatt.
  \]
  We shall refer to these as the transformed mass matrix and transformed
  stiffness matrix. Keeping $n$ fixed we are now interested in properties of
  the sequences $\left\{\Mmmat\right\}$ and $\left\{\Ammat\right\}$. First we
  observe the entries of the transformed mass matrix are
  \[
    \left(\Mmmat\right)_{ij} =
    \displaystyle\sum\limits_{k=0}^{m-1}
    \inner{\phi_i}{\varphi_k}\inner{\phi_j}{\varphi_k}.
  \]
  Assuming $m>n$ we expect $\left| \inner{\phi_i}{\varphi_k} \right|$ and
  $\left| \inner{\phi_j}{\varphi_k} \right|$ to be bounded by $k^{-2}$ for 
  $k>n$ so that the summed terms behave as $k^{-4}$ and finaly the entry
  is bounded by $m^{-3}$. Thus every entry in the matrix converges and we expect
  the series for $\left\{\Mmmat\right\}$ to converge. A similar argument can
  be made for entries of the transformed stiffness matrix. The difference is
  that the matrix $\Ammat$ introduces scaling of the eigenvalues which grow
  as $k^2$. As a result the entries should converge as $m^{-1}$. Next we shall
  make a more sound argument for existence of the limit.

  % Showing what Am, Mm really are (the projections)
  We consider entries of the transformed mass matrix again and write out
  \[
    \left(\Mmmat\right)_{ij} =
    \displaystyle\sum\limits_{k=0}^{m-1}
    \displaystyle\sum\limits_{l=0}^{m-1}
    \inner{\phi_i}{\varphi_k}\inner{\varphi_k}{\varphi_l}\inner{\phi_j}{\varphi_k}=
    \displaystyle\sum\limits_{k=0}^{m-1}
    \displaystyle\sum\limits_{l=0}^{m-1}
    \inner{\inner{\phi_i}{\varphi_k}\varphi_k}
          {\inner{\phi_j}{\varphi_l}\varphi_l}.
  \]
  The entries of transformed mass matrix are thus the inner products of the
  basis function of $V_n$ projected onto $V_m$.
  \[
    \left(\Mmmat\right)_{ij} = \inner{\pi_m\phi_i}
                                     {\pi_m\phi_j}.
  \]
  Writing out
  \[
    \left(\Ammat\right)_{ij} =
    \displaystyle\sum\limits_{k=0}^{m-1}
    \displaystyle\sum\limits_{l=0}^{m-1}
    \inner{\phi_i}{\varphi_k}
    \inner{\varphi_k^{\prime}}{\varphi_l^{\prime}}
    \inner{\phi_j}{\varphi_k}=
    \displaystyle\sum\limits_{k=0}^{m-1}
    \displaystyle\sum\limits_{l=0}^{m-1}
    \inner{\inner{\phi_i}{\varphi_k}\varphi_k^{\prime}}
    {\inner{\phi_j}{\varphi_l}\varphi_l^{\prime}}.
  \]
  we get that the entries of the transformed stiffness matrix are the $H^1_0$
  inner products of the projected basis functions of $V_n$,
  \[
    \left(\Ammat\right)_{ij} = \inner{\left\{\pi_m\phi_i\right\}}
    {\left\{\pi_m\phi_j\right\}}.
  \]
  Motivated by 
  $\norm{\pi_m \phi_i - \phi_i}_i \xrightarrow[]{m\rightarrow\infty}0$ and
  $i=0, 1$ we now ask if $\norm{\Mmat - \Mmmat}_{F}$ and 
  $\norm{\Amat - \Ammat}_{F}$ go to zero as $m$ goes to infinity. Note that
  for general matrix $\Bmat\in\R^{m\times m}$, the Frobenius norm is
  \[
    \norm{\Bmat}_{F} = \sqrt{
    \displaystyle\sum\limits_{i=0}^{m-1}
    \displaystyle\sum\limits_{j=0}^{m-1}
    \left|\Bmat_{ij}\right|^2
    }
  \]

  % measuting frobenius for M   [need to find both rate for m^-2*m^-2 and why
  % loose m
  The entries of the matrix $\Mmat-\Mmmat$ are $\left(\Mmat-\Mmmat\right)_{ij}=
  \inner{\phi_i}{\phi_j}-\inner{\pi_m\phi_i}{\pi_m\phi_j}$. By properties of
  $\pi_m$ we have $\pi_m\phi_i - \phi_i \perp V_m$ and since $\pi_m\phi_j\in V_m$
  we get that $\inner{\phi_i}{\pi_m\phi_j}=\inner{\pi_m\phi_i}{\pi_m\phi_j}$.
  Consequently $\inner{\phi_i}{\phi_j}-\inner{\pi_m\phi_i}{\pi_m\phi_j} =
  \inner{\phi_i-\pi_m\phi_i}{\phi_j - \pi_m\phi_j}$ and thus
  \[
    \left(\Mmat-\Mmmat\right)_{ij} = \inner{\phi_i-\pi_m\phi_i}{\phi_j - \pi_m\phi_j}
  \]
  \TODO{This seems to be $m^{-2}m^{-2}$, where do we loose one order?}. We
  can thus write $M=\displaystyle\lim_{m\to\infty}M_m$.
  % measuting frobenius for M   [need to find both rate for m^-1*m^-1 and why
  % loose m
  In a similar way we can prove that the entries of the matrix $\Amat-\Ammat$
  are
  \[
    \left(\Amat-\Ammat\right)_{ij} =
    \inner
    {\left(\phi_i-\pi_m\phi_i\right)^{\prime}}
    {\left(\phi_j - \pi_m\phi_j\right)^{\prime}}.
  \]
  The required orthogonality of $\left(\pi_m\phi_i - \phi_i\right)^{\prime}$ to
  $\left\{\varphi^{\prime}_j\right\}_{j=0}^{m-1}$ follows from  
  $\inner{\varphi_i^{\prime}}{\varphi_j^{\prime}}=\lambda_j\delta_{ij}$.
  \TODO{This seems to be $m^{-1}m^{-1}$, where do we loose one order?}. We
  can then write $A=\displaystyle\lim_{m\to\infty}A_m$.

  The above estimates were tested numerically with results summarized in Figure
  \ref{fig:eig_mat} and Table \ref{tab:eig_mat}. Figure \ref{fig:eig_mat} shows
  convergence of the matrix
  approximations. It is apparent from the figure that similar to expansion
  coefficients there exists for each $n$ a critical size of space $V_m$ which
  guarantees convergence of the transformed matrices. For spaces larger than
  the critical one, the rate of convergence agrees well with the analysis. 
  \begin{figure}
  \begin{center}
    \includegraphics[width=0.45\textwidth]{img/eig_A.pdf}
    \includegraphics[width=0.45\textwidth]{img/eig_M.pdf}
  \end{center}
  \label{fig:eig_mat}
  \caption{Convergence rate of matrix approximation. (left) The transformed
  stiffness matrix converges to finite element stiffness as $m{-1}$ once the
  space $V_m$ is sufficiently large. The critical size appears to be $n$.
  (right) The transformed mass matrix converges to finite element mass matrix
  as $m^{-3}$.
  }
  \end{figure}
  Table \ref{tab:eig_mat} shows the magnitude of the error in matrix
  approximations for $m=8192$ fixed and varying $n$. The error seems to grow
  as $n^2$. \TODO{Why?}
  \begin{table}
    \begin{center}
    \begin{tabular}{|c||c|c|c|c|c|}
      \hline
$n$ &  4& 16& 64& 256& 1024\\
      \hline\hline
$\norm{\Amat-\Ammat}_F$ & 2.02E-03& 2.81E-02& 4.18E-01& 6.51E+00& 1.03E+02 \\
      \hline
$\norm{\Mmat-\Mmmat}_F$ & 4.08E-12& 5.65E-11& 8.41E-10& 1.30E-08& 2.02E-07 \\
      \hline
    \end{tabular}
  \caption{The errors of matrix approximation for $m=8192$ and different $n$.}
  \label{tab:eig_mat}
  \end{center}
  \end{table}

  Finally we turn attention to properties of the transformation matrix $\Pmat$.
  Recall that in general, this is an $n$ by $m$ matrix which represents a
  transformation from the space of eigenfunction $V_m$ to the finite element 
  space $V_n$. Matrix $\Pmatt$ then represents a transformation from $V_n$ 
  to $V_m$. Note that by $\Mmmat=\mathbb{I}_m$ we know that the product 
  $\Pmat\Pmatt$ should scale as the finite element mass matrix. \ASK{This
  one has a condition number converging to 3. Why? And can we conclude already
here that $\Pmat$ will inherit the scaling?} Ideally we would have
$\Pmatt\Pmat=\mathbb{I}_n$. Observations about the matrix are in Figure
\ref{fig:eig_P} which shows dependence of condition number $\Pmat$ on $m$
  for different $n$ fixed. Note that until the space $V_m$ reaches the critical
  size $m=n$, the condition number increases. At the critical size, there is a
  turning point and the condition number converges to \TODO{(mysterious)} value
  of $\sqrt{3}$.
  
  \begin{figure}
  \begin{center}
    \includegraphics[width=0.5\textwidth]{img/eig_Pmn.pdf}
  \end{center}
  \label{fig:eig_P}
  \caption{Condition number of the transformation matrix $\Pmat$ for different $n$.
    Note that the matrix is rectangular in general.}
  \end{figure}
  For the special case where spaces $V_m$ and $V_n$ are inflated together the
  condition number of the square transformation matrix $\Pmat$ is shown in
  Table \ref{tab:eig_P}. Note that the value correspond to value of maximal
  condition number of the rectangular matrices in Figure $\ref{fig:eig_P}$. 
  The table indicates that this value is bounded by a constant $\approx 2.5$.  
  \TODO{Why?}
  \begin{table}
    \begin{center}
    \begin{tabular}{|c||c|c|c|c|c|c|c|c|}
      \hline
      $n$           &   64&  128&  256&  512& 1024& 2048& 4096& 8192\\
      \hline\hline
      $\kappa(\Pmat)$ & 2.39& 2.43& 2.45& 2.46& 2.46& 2.46& 2.47& 2.47\\
      \hline
      $\kappa(\Pmatt\Pmat)$ & 5.73& 5.90& 5.99& 6.04& 6.06& 6.08& 6.08&
      6.09\\
      \hline
    \end{tabular}
  \caption{Condition number for the square transformation matrix $\Pmat$ and
  $\Pmatt\Pmat$.}
  \label{tab:eig_P}
  \end{center}
  \end{table}
  \ASK{From FEM to Eigen I could go by projection that is $\Pmat$ or the
  interpolation. Is there a difference?}
  % Maybe things can be discussed in the light of condition numbers of matrices
  % A, M etc.

 \section{Legendre $\rightarrow$ FEM}
 Consider the finite dimensional space $V_m=\spn\left\{\psi_i\right\}_{i=0}^{m-1}$
 where $\psi_i$ are the functions due to \cite{shen_leg} and function $f$
 which has zeros on the boundary and is at least $C^0$. To approximate $f$ on
 $V_m$ we define $\pi_m f\in V_m$,
 as
 \[
   \pi_m f = \displaystyle\sum\limits_{k=0}^{m-1}c_k \psi_k,
 \]
 where the vector of expansion coefficients is given as a solution to linear
 system $\Mmmat c=\bvec$, with $\Mmmat$ the mass matrix on $V_m$ and
 $\bvec_i=\inner{f}{\psi_j}$. As an alternative to the $L^2$ projection we
 could consider another mapping which would take advantage of the orthogonality
 properties of function $\psi_j$.
 We write symbolically $f=\sum_k c_k\psi_k$ and consider a derivative of the
 expression. Using orthonormality of basis functions with respect to the
 $H^1_0$ inner product we get $\inner{f^{\prime}}{\psi^{\prime}_j} =
 \sum_k c_k\inner{\psi^{\prime}_k}{\psi^{\prime}_j}=c_k$. Consequently
 we shall define $\Pi_m f\in V_m$ as
 \[
   \Pi_m f = \displaystyle\sum\limits_{k=0}^{m-1}c_k \psi_k,
 \]
 with $c_k=\inner{f^{\prime}}{\psi^{\prime}_k}$. Note that for the
 eigenfunctions $\varphi_k$ the two projections yield the same object since
 \[
   \inner{f}{\varphi_k} =
   -\frac{1}{\lambda_k}\inner{f}{\varphi^{\prime\prime}_k} =
   \frac{1}{\lambda_k}\inner{f^{\prime}}{\varphi_k^{\prime}}.
 \]
  Figure \ref{fig:shen_smooth_projection} shows that with increasing $m$ both
  projections $\pi_mf, \Pi_mf$ converge to smooth $f$ exponentially in both
  the $L^2$ and the energy norm. However for functions $f(x)=(x^4-1)e^{2x}\sin{4\pi x}$
  the exponential convergence requires sufficiently large space $V_m$.
  \TODO{Theory for this and what to expect from hat?}
  % projection -> smooth
  \begin{figure}
  \begin{center}
    \includegraphics[width=0.45\textwidth]{img/shen_L2proj.pdf}
    \includegraphics[width=0.45\textwidth]{img/shen_H10proj.pdf}
  \end{center}
  \label{fig:shen_smooth_projection}
  \caption{Convergence rate of $\pi f$,  $\Pi f$ for smooth functions.
    (left) Convergence rate in the $L^2$ norm for the $L^2$ and $H^1_0$
    projection. (right) Convergence rate in the $L^2$ norm for the $H^1_0$
    and $H^1_0$ projection. For function $f=(x^4-1)e^{2x}$ both projections
    yield an approximation which converges exponentially. For 
    $f=(x^4-1)e^{2x}\sin{4\pi x}$ exponentially converges is obtained only for
    sufficiently large space $V_m$.
  }
  \end{figure}
  
  % Move to hat functions. If \Pi is used formulat for hat function coefficients
  % Consider equidistant and as before with xi=0. Odd->0. Even. Consder h=1 and
  % do the math. Confirm with Plot. What happens when spacind. Show plot. 2n.
  % But analyzi exactly. DON't FORGET REFERENCE FOR GAMMA
  We now consider the approximation properties of $\Pi_m \phi_i, \pi_m \phi_i$,
  with $\phi_i$ the finite element hat function (\ref{eq:hat}). Having seen
  that computing the $L^2$ projection involves solving a linear system, we
  first focus on the the $\Pi_m$ projection where the expansion coefficients are
  obtained easily. Indeed we have
  \[
  c_k=\inner{\phi_i^{\prime}}{\psi_k^{\prime}} =
  \frac{1}{x_i - x_{i-1}}\left[\psi_k\right]_{x_{i-1}}^{x_i} +
  \frac{1}{x_{i+1} - x_{i}}\left[\psi_k\right]_{x_{i}}^{x_{i+1}}.
  \]
  As in the case previous section we gain some insight into the properties of the
  projection by studying the hat functions centered at the origin. Recall that
  the basis function $\psi_k$ is even/odd if $k$ is even/odd. Therefore only
  $c_{2l}, l=0, 1, 2, \dots$ coeffcients are nonzero. For function
  $(-1, 0, 1)$ the even coefficients are then given as
  \[
    c_{2l} = 2\psi_{2l}(0),
  \]
  which follows from $\psi_k(\pm 1)=0$ for any $k$. Recalling the definition
  for basis functions we have\\ $c_{2l}=\beta_{2l}\left(L_{2(l+1)}-
  L_{l}\right)\left(0\right)$ which upon using the relation between values of
  Legendre polynomials at zero to gamma function \TODO{$^\text{citation needed}$}
  becomes
  \[
    c_{2l} = \frac{\left(-1\right)^{l+1}}{\sqrt{\pi}}
             \frac{1}{\sqrt{8l + 6}}
             \frac{\Gamma\left(l+\tfrac{1}{2}\right)}{\Gamma\left(i+1\right)}
             \frac{2l+\tfrac{3}{2}}{l+1}.
  \]
  Using the assymptotic expansion for the ratio of gamma functions given
  in \cite{tricomi} we get that the terms in the expression behave as
  $k^{-1/2}\cdot k^{-1/1}\cdot1$ and thus we expect the magnitude of coefficients to
  be bounded by $k^{-1}$. This prediction agrees well with the experiment
  (cf. left pane in Figure \ref{fig:shen_hat_projection}).
  
  The even expansion coefficients of more general hat functions $(-h, 0, h)$
  are given by
  \[
    c_{k} = \frac{2}{h}\left(\psi_k\left(0\right)- \psi_k\left(h\right)\right).
  \]
  For several values of $h$ the coeffcients' magnitue is shown in the right pane
  of Figure \ref{fig:shen_hat_projection}. We observe that for wavenumbers
  $k>\text{dim}V_n$ (note that the hat function $(-h, 0, h)$ is in the space
  $V_{\frac{2}{h}-1}$) the coefficients are bounded by $k^{-1}$. The
  approximation properties of the Shen's basis for the hat functions are thus
  worse than those of the eigenfunctions. We also note that for the
  eigenfunctions we had an estimate of the critical wavenumber $k_0=n+1$ which
  was in good agreement with the experiment. Here, the critical number appears
  to grow faster than linear.
  \begin{figure}
  \begin{center}
    \includegraphics[width=0.45\textwidth]{img/shen_hat_1.pdf}
    \includegraphics[width=0.45\textwidth]{img/shen_hat_h.pdf}
  \end{center}
  \label{fig:shen_hat_projection}
  \caption{Expansion coefficients of $\Pi_m$ projection of the hat functions
    $(-h, 0, h)$. (left) Special case $h=1$ confirms that the coefficients 
    are bound by $k^{-1}$. (right) The $k^{-1}$ decay occurs only for
    sufficiently large wavenumbers.}
  \end{figure}

  % Heuristics for matrices M, A
  We shall now use the projection $\Pi_m$ to construct approximations to finite
  element matrices. To this end we denote $V_n\subset V$ the finite dimensional
  subspace of $V$ constructed by $n$ hat functions $\phi_i$ and $V_m\subset V$
  the finite dimensional subspace spanned by the first $m$ polynomials
  $\psi_j$. Also we denote $\Amat, \Mmat \in \R^{n \times n}$
  the stiffness and mass matrices assembled on $V_n$ and let
  $\Ammatt, \Mmmatt \in \R^{m\times m}$ denote the stiffness and mass matrices
  assembled on $V_m$. Finally, we define a transformation matrix
  $\Pmat\in\R^{n \times m}$ that has expansion coefficients of $\Pi_m\phi_i$
  as its rows and transformed mass and stiffness matrices as
  \[
    \Mmmat = \Pmat\Mmmatt\Pmatt
    \quad{\text{ and }}\quad
    \Ammat = \Pmat\Ammatt\Pmatt.
  \]
  
  As in the previous section we begin the discussion of approximation properties
  of the transformed matrices by giving a heuristic argument for the expected
  order of convergence for the matrix entries. Recalling that the stiffness matrix
  over $V_m$ is simply an identity, we have for the entry of a transformed
  stiffness matrix 
  \[
    \left(\Ammat\right)_{ij} =
    \displaystyle\sum\limits_{k=0}^{m-1}
    \inner{\phi^{\prime}_i}{\psi^{\prime}_k}
    \inner{\phi^{\prime}_j}{\psi^{\prime}_k}.
  \]
  That is, the entries of the matrix are products of entries in the 
  transformation matrix which as we have shown decay at least as $k^{-1}$ for
  once the space $V_m$ is sufficiently large. Further we can expect the summation
  loop to strip one order and arrive at the expected bound $m^{-1}$. In case of
  the transformed mass matrix, we have the entries given as
  \[
    \left(\Mmmat\right)_{ij} =
    \displaystyle\sum\limits_{k=0}^{m-1}
    \left(\Pmat\right)_{ik}
    \left(\Mmmatt\right)_{kl}
    \left(\Pmat\right)_{jl}.
  \]
  Using (\ref{eq:shen_mat}) we have that the nonzero entries of the mass matrix
  $\Mmmatt$ scale as $k^{-2}$ yielding $k^{-4}$ for the summed terms. Consequently
  we expect the entries to converge at least as $m^{-3}$. The obtained bounds
  are noteworthy since they imply that for matrix entries the approximation
  properties of polynomials are the same as properties of the eigenfunctions
  despite the eigenfunctions' superiority in approximations of the hat functions.
  Our argument indicated that in both cases this is due to scaling that is
  introduced by the matrices $\Mmmatt, \Ammatt$.

  % Showing what Am, Mm really are (the projections)
  Having discussed convergence of matrix entries we shall now study convergence
  of the matrix approximations. To this end we consider
  \[
  \norm{\Amat-\Ammat}_F\quad\text{ and }\quad\norm{\Mmat-\Mmmat}_F.
  \]
  Following the ideas of previous section, it is easy to show that the entries
  of the transformed stiffness matrix are  
  \[
    \left(\Ammat\right)_{ij} =
    \inner{\left(\Pi_m\phi_i\right)^{\prime}}
    {\left(\Pi_m\phi_j\right)^{\prime}}
  \]
  while the entries of the transformed mass matrix read
  \[
    \left(\Mmmat\right)_{ij} = \inner{\Pi_m\phi_i}
                                     {\Pi_m\phi_j}.
  \]
  In the Frobenius norm we thus deal with terms
  \[
    \inner{\phi^{\prime}_i}{\phi^{\prime}_j} - 
    \inner{\left(\Pi_m\phi_i\right)^{\prime}}
          {\left(\Pi_m\phi_j\right)^{\prime}}
    \quad\text{ and }\quad
    \inner{\phi_i}{\phi_j} - \inner{\Pi_m\phi_i}{\Pi_m\phi_j}.
  \]
  The orthogonality of the $\Pi_m$ projection in the $H^1_0$ inner product
  implies $\inner{\psi^{\prime}_k}{\phi^{\prime}_i}=
  \inner{\psi^{\prime}_k}{\left(\Pi_m\phi_i\right)^{\prime}}
  $ for all $\psi_k\in V_m$ and as a consequence we get
  \[
    \left(\Amat-\Ammat\right)_{ij} =
    \inner
    {\left(\phi_i - \Pi_m\phi_i\right)^{\prime}}
    {\left(\phi_j - \Pi_m\phi_j\right)^{\prime}}.
  \]
  \TODO{This seems to be $m^{-1}m^{-1}$, where do we loose one order?}. We
  can thus write $\Amat=\displaystyle\lim_{m\to\infty}\Ammat$. The argument
  for the mass matrix approximation is based on the following observation.
  Suppose $\phi_i=\sum_k\inner{\phi_i^{\prime}}{\psi^{\prime}_k}\psi_k$ and take
  the inner product with $\Pi_m\phi_j$. Functions $\psi_k$ are not orthogonal but
  for $l$ fixed the inner product of $\psi_l$ with $\psi_k$ is
  nonzero only for $k\in\left\{l-2, l, l+2\right\}\cap\mathbb{N}$. It follows
  that $\inner{\phi_i, \Pi_m\phi_j}=\inner{\phi_i}{\Pi_{m+2}\phi_j}$ and in
  turn
  \[
  \left|
  \inner{\phi_i-\Pi_m\phi_i}{\phi_j - \Pi_m\phi_j}
  \right|
  \leq
  \left|
  \inner{\phi_i}{\phi_j}-\inner{\Pi_m\phi_i}{\Pi_m\phi_j}
  -\inner{\sum\limits_{k=m}^{m+1}\inner{\phi_j^{\prime}}{\psi^{\prime}_k}\psi_k}{\Pi_m\phi_i}
  -\inner{\sum\limits_{k=m}^{m+1}\inner{\phi_i^{\prime}}{\psi^{\prime}_k}\psi_k}{\Pi_m\phi_j}
  \right|.
  \]
  Thus we get 
  \[
    \left|\left(\Mmat-\Mmmat\right)_{ij}\right| \leq
  \left|
  \inner{\phi_i-\Pi_m\phi_i}{\phi_j - \Pi_m\phi_j}
  \right|
  \]
  \TODO{Same problem as always how to make step from here to the final rate?}

  To test the above estimates we performed convergence study. The results
  are summarized in \ref{fig:shen_mat} and Table \ref{tab:shen_mat}. We note
  that in our current implementation evaluating high order Legendre polynomials
  leads to instabilities and therefore we only considered spaces $V_m$ with
  dimensions up to 256. Figure \ref{fig:shen_mat} confirms the proposed rates
  for spaces $V_m$ sufficiently large. We note that similar to the expansion
  coefficients ``sufficienly large'' seems larger than $n$.
  Figure \ref{fig:eig_mat} shows
  \begin{figure}
  \begin{center}
    \includegraphics[width=0.45\textwidth]{img/shen_A.pdf}
    \includegraphics[width=0.45\textwidth]{img/shen_M.pdf}
  \end{center}
  \label{fig:shen_mat}
  \caption{Convergence rate of matrix approximation. (left) The transformed
    stiffness matrix converges to finite element stiffness as $m^{-1}$ once the
  space $V_m$ is sufficiently large. The critical size appears to be $n$.
  (right) The transformed mass matrix converges to finite element mass matrix
  as $m^{-3}$.
  }
  \end{figure}
  Table \ref{tab:shen_mat} shows the magnitude of the error in matrix
  approximations for $m=256$ fixed and varying $n$. The error seems to grow
  as $n^2$. \TODO{Why?}
  \begin{table}
    \begin{center}
    \begin{tabular}{|c||c|c|c|c|c|}
      \hline
$n$ & 4& 16& 32& 48& 64 \\
      \hline\hline
$\norm{\Amat-\Ammat}_F$ & 9.49E-02& 1.32E+00& 5.24E+00& 9.71E+00& 2.45E+01 \\
      \hline
$\norm{\Mmat-\Mmmat}_F$ & 4.32E-07& 6.02E-06& 3.03E-05& 3.12E-05& 1.48E-04 \\
      \hline
    \end{tabular}
  \caption{The errors of matrix approximation for $m=256$ and different $n$.}
  \label{tab:shen_mat}
  \end{center}
  \end{table}

  We finally consider properties of the transformation matrix $\Pmat$. We remind
  the reader that the transformation matrix between the finite element space
  $V_n$ and the space spanned by the eigenfunctions $V_m$ had a condition number
  $\kappa$ bounded by a constant (cf. Figure \ref{fig:eig_P}, Table
  \ref{tab:eig_P}). For the square matrix we had the bound $\kappa\approx 2.5$
  and the matrix was thus easy to invert. Properties of the transformation
  matrix $\Pmat$ are summarized in Figure \ref{fig:shen_P} and
  Tables \ref{tab:shen_Pm}, \ref{tab:shen_P}. Figure \ref{fig:shen_P} shows
  that for given $n$ the condition number of $\Pmat$ grows with $m$ until the
  spaces $V_n$, $V_m$ have the same dimensions. Afterwards the condition number
  decreases to a constant value. This behaviour is qualitatively same as the
  behaviour of that of the transformation matrix between finite element space
  and the space of eigenfunctions but there are two key differences (i) for
  $m=n$ the condition number grows with exponentially with $n$, cf.
  Table \ref{tab:shen_P} (ii) the
  condition number for rectangular matrix with $m \gg n$ grows linearly with
  $n$, cf Table \ref{tab:shen_Pm}. The latter fact can perhaps be understood in
  the light of the fact that $\Pmat\Pmatt\rightarrow\Amat$ which has a condition
  number growing as $n^2$.

  \begin{figure}
  \begin{center}
    \includegraphics[width=0.5\textwidth]{img/shen_Pmn.pdf}
  \end{center}
  \label{fig:shen_P}
  \caption{Condition number of the transformation matrix $\Pmat$ for different $n$.
    Note that the matrix is rectangular in general.}
  \end{figure}

  \begin{table}
    \begin{center}
    \begin{tabular}{|c||c|c|c|c|c|c|c|}
      \hline
      $n$           & 16 & 24 & 32 & 40 & 48 & 56 & 64\\
      \hline\hline
      $\kappa\left(\Pmat\right)$ & 10.59 & 15.43 & 20.27 & 25.04 & 29.81 & 34.54 & 39.30 \\
      \hline
    \end{tabular}
  \caption{Condition number of rectangular transformation matrix $\Pmat$ for
  $m=256$.}
  \label{tab:shen_Pm}
  \end{center}
  \end{table}
  Our results show that the approximation properties of the matrices due to
  eigenfunctions are the same as the approximation properties of matrices from
  the Shen's polynomials. From this point of view, the basis of eigenfunctions
  is equivalent to basis of modified Legendre polynomials. On the other hand
  properties of the transformation between these basis and the finite element
  space differ. In case of the eigenfunctions the transformation is bounded
  regardless of the size of the spaces. For the polynomials, the transformation
  is well behaved if the polynomial space has greater dimension than the finite
  element space. In case the spaces are of the same size the transformation is
  unbounded.
  \begin{table}
    \begin{center}
    \begin{tabular}{|c||c|c|c|c|c|}
      \hline
      $n$           &4& 16& 32& 64& 128 \\
      \hline\hline
      $\kappa(\Pmat)$ & 1.90E+00& 9.39E+02& 2.50E+07& 1.02E+17& 4.36E+16\\
      \hline
      $\kappa(\Pmatt\Pmat)$ & 3.63E+00& 8.82E+05& 6.48E+14& 1.35E+17& 4.55E+17\\
      \hline
    \end{tabular}
  \caption{Condition number for the square transformation matrix $\Pmat$ and
  $\Pmatt\Pmat$.}
  \label{tab:shen_P}
  \end{center}
  \end{table}
  %TODO There should be some comment on the $L^2$ projection
  %TODO curved arrow, and labels for maps
  \begin{figure}
    \begin{center}
      \includegraphics[width=0.5\textwidth]{graph.1}
    \caption{Graph}
    \label{fig:graph}
    \end{center}
  \end{figure}
  
  Let us review our progress so far. Looking at Figure (\ref{fig:graph}) we
  see that we have built edges between the finite element space 
  and the spaces of eigenfunctions and polynomials. The figure does not seem
  complete and clearly what is missing is the connection between the
  eigenfunctions and the polynomials. We shall establish such connections in the
  next two sections.

  \section{Legendre $\rightarrow$ Eigen}
  To any function from $V_n=\spn\left\{\varphi_i\right\}_{i=0}^{n-1}$ we shall
  assign function $\Pi_m\varphi_i\in V_m=\spn\left\{\psi_i\right\}_{i=0}^{m-1}$.
  As in the previous section we let
  \[
    \Pi_m\varphi_i=\displaystyle\sum\limits_{k=0}^{m-1}c_k\psi_k
    \quad\text{ with }\quad
    c_k = \inner{\varphi^{\prime}_i}{\psi^{\prime}_k}.
  \]
  Observe that $c_k=\lambda_i\inner{\varphi_i}{\psi_k}$. The coefficients
  of $\Pi_m$ projection can thus be computed from coefficients of Fourier
  series of Legendre polynomials for which \cite{arxiv} give the following
  formula
  \[
    \inner
    {L_k}{e^{\imath\lambda x}}=\i^k\sqrt{\frac{2\pi}{\lambda}}J_{k+1/2}(\lambda).
  \]
  Using assymptotics of the Bessel function \TODO{$^{\text{citation needed}}$}
  $J_k(x)\propto\frac{1}{2\pi k}\left(\frac{e\lambda}{2k}\right)^{k}$ we see
  that for the wave number $k>k0$ where $k_0=\frac{e\lambda-1}{2}$ the expansion
  coefficients decrease exponentially. In turn $\phi_i - \Pi_m\phi_i$ decreases
  exponentially for $m>i$ in the $L^2$ or $H^1_0$ norms. Also note that
  the critical wavenumber $k_0$ grows linearly with the frequency. The
  approximation properties of the projection should translate into following
  properties of the matrix approximations:
  \begin{enumerate}
    \item $\norm{\Amat - \Ammat}$ decreases exponentially for $m>n$
    \item $\norm{\Mmat - \Mmmat}$ decreases exponentially for $m>n$
    \item $\kappa(\Pmat)=n$ 
  \end{enumerate}
  The first two points follow from ideas which should be sufficiently familiar
  by now. The reasoning for the last point is as follows. We have
  $\Ammatt=\mathbb{I}$, consequently $\Ammat=\Pmat\Pmatt$ but
  $\Pmat\Pmatt\rightarrow\Amat$. Further $\kappa(\Amat)\propto n^2$ from where
  \TODO{this is somewhere in Golub} $\kappa{\Pmat}\propto n$. Finally for two
  by two matrix $\Amat$ we have $\kappa(\Amat)=\pi^2/\frac{\pi^2}{4}$ and thus
  $\kappa(\Pmat)=n$.

  These conjectures agree with the numerical experiments. Figure \ref{fig:se_mat}
  confirms exponential decay of both matrix approximations. In Figure
  \ref{fig:se_P} we observe that the condition number of the transformation
  matrix converges to prediceted value of $2n$. This result is also highlighted
  in Table \ref{tab:se_P}. Further we observe that just as in the transformation
  to finite element space the square transoformation has a condition number
  which grows exponentially.

  \begin{figure}
  \begin{center}
    \includegraphics[width=0.45\textwidth]{img/seA.pdf}
    \includegraphics[width=0.45\textwidth]{img/seM.pdf}
  \end{center}
  \label{fig:se_mat}
  \caption{Convergence rate of matrix approximation. (left) The transformed
    stiffness matrix. (right) The transformed mass matrix.
    Both matrix approximation converge exponentially for $V_m>n$.
  }
  \end{figure}

  \begin{table}
    \begin{center}
    \begin{tabular}{|c||c|c|c|c|c|c|c|}
      \hline
      $n$ & 2& 4& 8& 16& 24& 32& 40\\
      \hline\hline
      $\Pmat$ & 2.00(16)& 4.00(24)& 8.00(32)& 16.00(64)& 24.00(72)& 32.00(96)&40.00(96) \\
      \hline
      $\mathbb{P}_{n}$ & 1.94E+00& 3.30E+00& 6.23E+00& 2.40E+01& 4.42E+02&
      9.13E+03& 2.01E+05 \\
      \hline
    \end{tabular}
  \caption{Condition number for the transformation matrix $\Pmat$.
    (row i) Condition number of $n\times m$. The $m$ value
    is shown in the corner. The condition number grows as $n$.
    (row ii) Condition number of square trasformation matrix grows
    exponentially.}
  \label{tab:se_P}
  \end{center}
  \end{table}

  \begin{figure}
  \begin{center}
    \includegraphics[width=0.5\textwidth]{img/sePmn.pdf}
  \end{center}
  \label{fig:se_P}
  \caption{Condition number of the transformation matrix $\Pmat$ for different $n$.
    Note that the matrix is rectangular in general.}
  \end{figure}

  \section{Eigen $\rightarrow$ Legendre}
  % I read something about this being analytical
  % interpolation in from/to both spaces

  % We will do shen->eigen and shen<-eigen
  %
  %

  % Why is cond(M_fem)=3
  % What is the pcolor of P matrices
  

  % When you are finish, you have a draft.
  % Print out and read: correcting mistakes but mostly getting new questions.
  % Then try to answer all the questions
  % Most likely revise notation
  % Rewrite send to kent
  % Can we put the paper into some framework?
  % If yes, great, do it, polish and ta da.
  
  % Shen does multiple eigenvalues at once?

  \bibliographystyle{plain}
  \bibliography{CG_matrices}
\end{document}
